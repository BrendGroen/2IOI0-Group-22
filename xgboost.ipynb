{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file we train a XGboost model on our data on the BPI_Challenge_2012 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from aux_functions import split_data\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/preprocessed/BPI_Challenge_2012.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining what category each column fall into\n",
    "dropping_columns = ['time:timestamp', 'case:REG_DATE', 'case:concept:name', 'next_timestamp']\n",
    "numerical_columns = ['org:resource', 'case:AMOUNT_REQ', 'position']\n",
    "categorical_columns = ['lifecycle:transition', 'concept:name']\n",
    "target_column = 'next_concept:name'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target column to category\n",
    "original_target = df[target_column].copy()\n",
    "df[target_column] = df[target_column].astype('category')\n",
    "df[target_column] = df[target_column].cat.codes\n",
    "\n",
    "# One-hot encoding of the categorical columns \n",
    "df = pd.get_dummies(df, columns=categorical_columns)\n",
    "\n",
    "# Columns that were one_hot_encoded\n",
    "one_hot_encoded_columns = [col for col in df.columns if (col not in numerical_columns) and (col != target_column) and (col not in dropping_columns)]\n",
    "one_hot_encoded_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into train and test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "train_df, test_df = split_data(df, ratio=0.8, report=True)\n",
    "X_train = train_df[numerical_columns + one_hot_encoded_columns]\n",
    "y_train = train_df[target_column]\n",
    "X_test = test_df[numerical_columns + one_hot_encoded_columns]\n",
    "y_test = test_df[target_column]\n",
    "\n",
    "# Getting the feature types for xgboost\n",
    "ft = ['q' if feature in numerical_columns else 'c' for feature in X_train.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DMatrix for training and testing\n",
    "train_dmatrix = xgb.DMatrix(X_train, y_train, feature_types=ft, enable_categorical=True)\n",
    "test_dmatrix = xgb.DMatrix(data=X_test, label=y_test, feature_types=ft, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XGBoost parameters\n",
    "params = {\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    'objective': 'multi:softmax',  # for multi-class classification\n",
    "    'num_class': len(df[target_column].unique()),  # number of classes\n",
    "    'eval_metric': 'merror' # evaluation metric\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the XGBoost model\n",
    "xgb_model = xgb.train(params, train_dmatrix, num_boost_round=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the result and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "predictions = xgb_model.predict(test_dmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the f1, precision, recall, and accuracy\n",
    "f1 = f1_score(y_test, predictions, average='weighted')\n",
    "precision = precision_score(y_test, predictions, average='weighted')\n",
    "recall = recall_score(y_test, predictions, average='weighted')\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(f'F1: {f1:.5f}, Precision: {precision:.5f}, Recall: {recall:.5f}, Accuracy: {accuracy:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see if the model is overfitting by comparing the training and testing error\n",
    "train_predictions = xgb_model.predict(train_dmatrix)   \n",
    "f1 = f1_score(y_train, train_predictions, average='weighted')\n",
    "precision = precision_score(y_train, train_predictions, average='weighted')\n",
    "recall = recall_score(y_train, train_predictions, average='weighted')\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "\n",
    "print(f'Training F1: {f1:.5f}, Training Precision: {precision:.5f}, Training Recall: {recall:.5f}, Training Accuracy: {train_accuracy:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mapping with the orginal_target and encoded one\n",
    "mapping = dict(zip(df[target_column], original_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting a single instance \n",
    "position = 45\n",
    "single_instance = X_test.iloc[position].values.reshape(1, -1)\n",
    "single_instance_dmatrix = xgb.DMatrix(data=single_instance, feature_names=X_test.columns, feature_types=ft, enable_categorical=True)\n",
    "single_instance_prediction = xgb_model.predict(single_instance_dmatrix)\n",
    "\n",
    "# Finding the mapping of the prediction to the actual value\n",
    "print(f'Prediction: {mapping[single_instance_prediction[0]]},\\nActual: {mapping[y_test.iloc[position]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting a single instance \n",
    "position = 600\n",
    "single_instance = X_test.iloc[position].values.reshape(1, -1)\n",
    "single_instance_dmatrix = xgb.DMatrix(data=single_instance, feature_names=X_test.columns, feature_types=ft, enable_categorical=True)\n",
    "single_instance_prediction = xgb_model.predict(single_instance_dmatrix)\n",
    "\n",
    "# Finding the mapping of the prediction to the actual value\n",
    "print(f'Prediction: {mapping[single_instance_prediction[0]]},\\nActual: {mapping[y_test.iloc[position]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the feature importance of the top 10 features\n",
    "\n",
    "xgb.plot_importance(xgb_model, max_num_features=10, importance_type='weight')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(20, 20)\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.ylabel('Feature Names', fontsize=15)\n",
    "plt.xlabel('Feature Importance', fontsize=15)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.title('Contribution of each Feature to the Model XGBoost 2012')\n",
    "plt.savefig('figs/feature_importance_XGboost_2012.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
