{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import statsmodels.api as sm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pm4py.read_xes('data/extracted/BPI_Challenge_2012.xes')\n",
    "df = pm4py.convert_to_dataframe(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translating the Dutch phrases in the 'concept:name' column to English\n",
    "translation_dict = {\n",
    "    'W_Completeren aanvraag': 'W_Complete request',\n",
    "    'W_Nabellen offertes': 'W_Follow up quotes',\n",
    "    'W_Nabellen incomplete dossiers': 'W_Follow up incomplete files',\n",
    "    'W_Valideren aanvraag': 'W_Validate request',\n",
    "    'W_Afhandelen leads': 'W_Handle leads',\n",
    "    'A_SUBMITTED': 'A_SUBMITTED',\n",
    "    'A_PARTLYSUBMITTED': 'A_PARTLYSUBMITTED',\n",
    "    'A_DECLINED': 'A_DECLINED',\n",
    "    'A_PREACCEPTED': 'A_PREACCEPTED',\n",
    "    'O_SENT': 'O_SENT',\n",
    "    'O_CREATED': 'O_CREATED',\n",
    "    'O_SELECTED': 'O_SELECTED',\n",
    "    'A_ACCEPTED': 'A_ACCEPTED',\n",
    "    'A_FINALIZED': 'A_FINALIZED',\n",
    "    'O_CANCELLED': 'O_CANCELLED',\n",
    "    'O_SENT_BACK': 'O_SENT_BACK',\n",
    "    'A_CANCELLED': 'O_CANCELLED',\n",
    "    'A_REGISTERED': 'A_REGISTERED',\n",
    "    'A_ACTIVATED': 'A_ACTIVATED',\n",
    "    'A_APPROVED': 'A_APPROVED',\n",
    "    'O_ACCEPTED': 'O_ACCEPTED',\n",
    "    'O_DECLINED': 'O_DECLINED',\n",
    "    'W_Beoordelen fraude': 'W_Evaluate fraud',\n",
    "    'W_Wijzigen contractgegevens': 'W_Modify contract details'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['concept:name_eng'] = df['concept:name'].map(translation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding position to the dataframe\n",
    "df['position'] = df.groupby('case:concept:name').cumcount() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the next activity(concept:name) to the dataframe and if the next activity is not available, then it will be fill in with No_Activity\n",
    "df['next_activity'] = df.groupby('case:concept:name')['concept:name'].shift(-1).fillna('No_Activity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['case:REG_DATE', 'lifecycle:transition'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('baseline_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date1 = pd.to_datetime(df['time:timestamp'], errors='coerce', format='%Y-%m-%d %H:%M:%S.%f%z')\n",
    "date2 = pd.to_datetime(df['time:timestamp'], errors='coerce', format='%Y-%m-%d %H:%M:%S%z')\n",
    "df['date'] = date1.fillna(date2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date_milliseconds\"] = (df['date'] - pd.Timestamp(\"1970-01-01\", tz = \"UTC\")) // pd.Timedelta('1ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"duration\"] = df.groupby(\"case:concept:name\")[\"date_milliseconds\"].shift(-1) - df[\"date_milliseconds\"]\n",
    "\n",
    "#df[\"duration\"] = df[\"duration\"] / (1000 * 60 * 60)  # Convert milliseconds to hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the average duration for each position\n",
    "position_dict_time = {}\n",
    "for j in range(1, 176):\n",
    "    time = []\n",
    "    for i in (df[df['position'] == j]).index:\n",
    "        time.append(df[\"duration\"].iloc[i])\n",
    "    if len(time) > 0:\n",
    "        position_dict_time[j] = sum(time)/len(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the average duration to each position\n",
    "\n",
    "df['predicted_next_timestamp_milliseconds'] = df['date_milliseconds'] + df['position'].map(lambda x: position_dict_time[x]).fillna(0)\n",
    "#df[\"duration\"] = df['position'].map(lambda x: position_dict_time[x]).fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_next_timestamp'] = df['predicted_next_timestamp_milliseconds'].apply(lambda x: '%d' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the predicted_timestamp to datetime\n",
    "df['predicted_next_timestamp'] = pd.to_datetime(df['predicted_next_timestamp'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['next_timestamp'] = df.groupby('case:concept:name')['time:timestamp'].shift(-1).fillna(0)\n",
    "df['next_timestamp_milliseconds'] = df.groupby('case:concept:name')['date_milliseconds'].shift(-1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['next_timestamp_milliseconds'] = df['next_timestamp_milliseconds'].apply(lambda x: '%d' % x)\n",
    "df['predicted_next_timestamp_milliseconds'] = df['predicted_next_timestamp_milliseconds'].apply(lambda x: '%d' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"time_since_start\"] = df.groupby('case:concept:name')[\"duration\"].cumsum().shift(1).fillna(0)\n",
    "\n",
    "df[\"time_since_start\"] = df[\"time_since_start\"] / (1000*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"duration\"] = df[\"duration\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date_hours\"] = df[\"date_milliseconds\"] / (1000*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_upper_multiple(x, z = 24):\n",
    "    # Calculate the remainder when x is divided by z\n",
    "    remainder = x % z\n",
    "    \n",
    "    # If remainder is zero, x is already a multiple of z\n",
    "    if remainder == 0:\n",
    "        return x\n",
    "    \n",
    "    # Otherwise, calculate the nearest upper multiple\n",
    "    return x + z - remainder\n",
    "\n",
    "df[\"nearest_midnight\"] = df[\"date_hours\"].apply(lambda x: nearest_upper_multiple(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"time_until_midnight\"] = df[\"nearest_midnight\"] - df[\"date_hours\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hours_of_the_day\"] = 24 - df[\"time_until_midnight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"month\"] = df[\"date\"].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_conversion_dict = {1: \"January\",\n",
    "                         2: \"February\",\n",
    "                         3: \"March\",\n",
    "                         4: \"April\",\n",
    "                         5: \"May\",\n",
    "                         6: \"June\",\n",
    "                         7: \"July\",\n",
    "                         8: \"August\",\n",
    "                         9: \"September\",\n",
    "                         10: \"October\",\n",
    "                         11: \"November\",\n",
    "                         12: \"December\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"month_as_string\"] = df[\"month\"].map(month_conversion_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"day\"] = df['date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_conversion_dict = {0: \"Monday\",\n",
    "                       1: \"Tuesday\",\n",
    "                       2: \"Wednesday\",\n",
    "                       3: \"Thursday\",\n",
    "                       4: \"Friday\",\n",
    "                       5: \"Saturday\",\n",
    "                       6: \"Sunday\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"day_as_string\"] = df[\"day\"].map(day_conversion_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"time_elapsed_since_previous_event\"] = df[\"date_milliseconds\"] - df.groupby(\"case:concept:name\")[\"date_milliseconds\"].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"time_elapsed_since_previous_event\"] = df[\"time_elapsed_since_previous_event\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"next_timestamp_milliseconds\"] = df[\"next_timestamp_milliseconds\"].astype(float)\n",
    "\n",
    "df[\"next_timestamp_hours\"] = df[\"next_timestamp_milliseconds\"] / (1000*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the data to one-hot encoded format\n",
    "encoded_array_month = encoder.fit_transform(df[[\"month_as_string\"]]).toarray()\n",
    "\n",
    "# Get the feature names from the encoder's categories_\n",
    "feature_names = encoder.categories_[0]\n",
    "\n",
    "# Create a new DataFrame with one-hot encoded columns and correct column names\n",
    "df_encoded = pd.DataFrame(encoded_array, columns=feature_names)\n",
    "\n",
    "# Concatenate the original DataFrame with the one-hot encoded DataFrame\n",
    "df = pd.concat([df, df_encoded], axis=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the data to one-hot encoded format\n",
    "encoded_array = encoder.fit_transform(df[[\"day_as_string\"]]).toarray()\n",
    "\n",
    "# Get the feature names from the encoder's categories_\n",
    "feature_names = encoder.categories_[0]\n",
    "\n",
    "# Create a new DataFrame with one-hot encoded columns and correct column names\n",
    "df_encoded = pd.DataFrame(encoded_array, columns=feature_names)\n",
    "\n",
    "# Concatenate the original DataFrame with the one-hot encoded DataFrame\n",
    "df = pd.concat([df, df_encoded], axis=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "\n",
    "encoded_array = encoder.fit_transform(df[[\"concept:name\"]]).toarray()\n",
    "\n",
    "\n",
    "feature_names = encoder.categories_[0]\n",
    "\n",
    "\n",
    "df_encoded = pd.DataFrame(encoded_array, columns=feature_names)\n",
    "\n",
    "\n",
    "df = pd.concat([df, df_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encode_activity_list = list(df_encoded.columns)\n",
    "linear_regression_extra_predictors_list = [\"position\", \"time_until_midnight\", \"month\", \"day\", \"time_elapsed_since_previous_event\", \"case:AMOUNT_REQ\", \"time_since_start\", \"next_timestamp_hours\", \"duration\"]\n",
    "one_hot_encode_activity_list.extend(linear_regression_extra_predictors_list)\n",
    "\n",
    "one_hot_encode_activity_list\n",
    "\n",
    "\n",
    "df_linear_regression = df[one_hot_encode_activity_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_linear_regression[['A_ACCEPTED', 'A_ACTIVATED', 'A_APPROVED', 'A_CANCELLED', 'A_DECLINED',\n",
    "       'A_FINALIZED', 'A_PARTLYSUBMITTED', 'A_PREACCEPTED', 'A_REGISTERED',\n",
    "       'A_SUBMITTED', 'O_ACCEPTED', 'O_CANCELLED', 'O_CREATED', 'O_DECLINED',\n",
    "       'O_SELECTED', 'O_SENT', 'O_SENT_BACK', 'W_Afhandelen leads',\n",
    "       'W_Beoordelen fraude', 'W_Completeren aanvraag',\n",
    "       'W_Nabellen incomplete dossiers', 'W_Nabellen offertes',\n",
    "       'W_Valideren aanvraag', 'W_Wijzigen contractgegevens', 'position',\n",
    "       'time_until_midnight', 'month', 'day',\n",
    "       'time_elapsed_since_previous_event', 'case:AMOUNT_REQ',\n",
    "       'time_since_start']]\n",
    "y = df_linear_regression[\"next_timestamp_hours\"]\n",
    "\n",
    "extra_var = df_linear_regression[[\"position\", 'A_ACCEPTED', 'A_ACTIVATED', 'A_APPROVED', 'A_CANCELLED', 'A_DECLINED',\n",
    "       'A_FINALIZED', 'A_PARTLYSUBMITTED', 'A_PREACCEPTED', 'A_REGISTERED',\n",
    "       'A_SUBMITTED', 'O_ACCEPTED', 'O_CANCELLED', 'O_CREATED', 'O_DECLINED',\n",
    "       'O_SELECTED', 'O_SENT', 'O_SENT_BACK', 'W_Afhandelen leads',\n",
    "       'W_Beoordelen fraude', 'W_Completeren aanvraag',\n",
    "       'W_Nabellen incomplete dossiers', 'W_Nabellen offertes',\n",
    "       'W_Valideren aanvraag', 'W_Wijzigen contractgegevens']]\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test, extra_var_train, extra_var_test = train_test_split(X, y, extra_var, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_main_variables = pd.DataFrame({'Position': extra_var_test[\"position\"], 'Actual': y_test, 'Predicted': y_pred})\n",
    "\n",
    "results_df_misc_variables = extra_var_test[['A_ACCEPTED', 'A_ACTIVATED', 'A_APPROVED', 'A_CANCELLED', 'A_DECLINED',\n",
    "       'A_FINALIZED', 'A_PARTLYSUBMITTED', 'A_PREACCEPTED', 'A_REGISTERED',\n",
    "       'A_SUBMITTED', 'O_ACCEPTED', 'O_CANCELLED', 'O_CREATED', 'O_DECLINED',\n",
    "       'O_SELECTED', 'O_SENT', 'O_SENT_BACK', 'W_Afhandelen leads',\n",
    "       'W_Beoordelen fraude', 'W_Completeren aanvraag',\n",
    "       'W_Nabellen incomplete dossiers', 'W_Nabellen offertes',\n",
    "       'W_Valideren aanvraag', 'W_Wijzigen contractgegevens']]\n",
    "\n",
    "\n",
    "results_df = pd.concat([results_df_main_variables, results_df_misc_variables], axis = 1)\n",
    "\n",
    "results_df[results_df[\"Actual\"] != 0]\n",
    "\n",
    "results_df[\"Predicted\"] = pd.to_datetime(results_df[\"Predicted\"], unit = \"h\")\n",
    "\n",
    "results_df[\"Actual\"] = pd.to_datetime(results_df[\"Actual\"], unit = \"h\")\n",
    "\n",
    "#decoded_df = results_df[['A_ACCEPTED', 'A_ACTIVATED', 'A_APPROVED', 'A_CANCELLED', 'A_DECLINED',\n",
    "#       'A_FINALIZED', 'A_PARTLYSUBMITTED', 'A_PREACCEPTED', 'A_REGISTERED',\n",
    "#       'A_SUBMITTED', 'O_ACCEPTED', 'O_CANCELLED', 'O_CREATED', 'O_DECLINED',\n",
    "#       'O_SELECTED', 'O_SENT', 'O_SENT_BACK', 'W_Afhandelen leads',\n",
    "#       'W_Beoordelen fraude', 'W_Completeren aanvraag',\n",
    "#       'W_Nabellen incomplete dossiers', 'W_Nabellen offertes',\n",
    "#       'W_Valideren aanvraag', 'W_Wijzigen contractgegevens']].idxmax(axis=1)\n",
    "\n",
    "#decoded_df\n",
    "\n",
    "results_df = results_df.groupby(\"Position\")[[\"Predicted\", \"Actual\"]].mean()\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.Series(model.coef_, index=X.columns)\n",
    "\n",
    "feature_importances = pd.DataFrame(feature_importances)\n",
    "\n",
    "feature_importances.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = results_df.plot(title = \"Comparing Actual Next Timestamps to Predicted Next Timestamps\")\n",
    "ax.set_xlabel(\"Position\")\n",
    "ax.set_ylabel(\"Next Timestamp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"2012_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The relevant code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series\n",
    "df.set_index(\"position\", inplace = True)\n",
    "plt.plot(df[\"duration\"])\n",
    "plt.title(\"Position vs Duration\")\n",
    "plt.xlabel(\"Position\")\n",
    "plt.ylabel(\"Duration Between Processes\")\n",
    "plt.show()\n",
    "df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot indicates a potential time trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adf_test = adfuller(df[\"duration\"])\n",
    "# Output the results\n",
    "#print('ADF Statistic: %f' % adf_test[0])\n",
    "#print('p-value: %f' % adf_test[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for key, value in adf_test[4].items():\n",
    "    #print('Critial Values:')\n",
    "    #print(f'   {key}, {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Number of Lags Used: %f' % adf_test[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal number of lags is 62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ADF test indicates that there is no stationarity in the data as the p-value is lower than 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also means that differencing will not be neecessary and that an ARMA model can be used instead of ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(df[\"duration\"], lags=62)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(df[\"duration\"], lags=62)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the ARIMA(1,0,1) model\n",
    "\n",
    "model = ARIMA(df[\"duration\"], order=(1, 0, 1))\n",
    "model_fit = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = pd.DataFrame(model_fit.resid)\n",
    "fig, ax = plt.subplots(1,2)\n",
    "residuals.plot(title=\"Residuals\", ax=ax[0])\n",
    "residuals.plot(kind='kde', title='Density', ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_fit.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df[\"duration\"], label='Actual Data', color='blue')\n",
    "plt.plot(predictions, label='Predictions', color='red')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Actual Data vs. ARIMA Predictions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)*0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicates the training data should end at position 33\n",
    "\n",
    "x = pd.DataFrame(df.groupby(df.index)[\"duration\"].count())\n",
    "\n",
    "x.iloc[0:33][\"duration\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing data\n",
    "\n",
    "filtered_df = df[(df.index >= 0) & (df.index <= 33)]\n",
    "filtered_df_2 = df[(df.index > 33)]\n",
    "\n",
    "train = pd.DataFrame(filtered_df[\"duration\"])\n",
    "test = pd.DataFrame(filtered_df_2[\"duration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "# model = ARIMA(train, order=(3,2,1))  \n",
    "model = ARIMA(train, order=(1, 0, 1))  \n",
    "fitted = model.fit()\n",
    "\n",
    "# Training prediction\n",
    "predictions = fitted.predict()\n",
    "\n",
    "# Forecast\n",
    "forecast = fitted.forecast(steps=len(test), index = test.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(12,5), dpi=100)\n",
    "plt.plot(train, label='training')\n",
    "plt.plot(test, label='actual')\n",
    "plt.plot(predictions, label='prediction')\n",
    "plt.title('Predictions (Training) vs Actual')\n",
    "plt.legend(loc='upper left', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(12,5), dpi=100)\n",
    "plt.plot(train, label='training')\n",
    "plt.plot(test, label='actual')\n",
    "plt.plot(forecast, label='forecast')\n",
    "plt.title('Forecast (Testing) vs Actual')\n",
    "plt.legend(loc='upper left', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Automatically evaluates the best model based on the AIC\n",
    "\n",
    "# Caution the code takes about 10 minutes to execute\n",
    "\n",
    "#from statsmodels.tsa.arima_model import ARIMA\n",
    "#import pmdarima as pm\n",
    "\n",
    "#model = pm.auto_arima(df[\"duration\"], start_p=1, start_q=1,\n",
    "#                      test='adf',       # use adftest to find optimal 'd'\n",
    "#                      max_p=3, max_q=3, # maximum p and q\n",
    "#                      m=1,              # frequency of series\n",
    "#                      d=None,           # let model determine 'd'\n",
    "#                      seasonal=False,   # No Seasonality\n",
    "#                      start_P=0, \n",
    "#                      D=0, \n",
    "#                      trace=True,\n",
    "#                      error_action='ignore',  \n",
    "#                      suppress_warnings=True, \n",
    "#                      stepwise=True)\n",
    "\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "mse_test = mean_squared_error(test, forecast)\n",
    "mape_test = mean_absolute_percentage_error(test, forecast)\n",
    "\n",
    "print(\"The mean squared error is: %f\" %mse_test)\n",
    "print(\"The mean absolute percentage error is: %f\" %mape_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating variance\n",
    "##variance = np.var(forecast)\n",
    "\n",
    "# Evaluating SSE\n",
    "#SSE = np.mean((np.mean(forecast) - test)** 2)  \n",
    "  # O/P : 85.03300391390214\n",
    "\n",
    "# Evaluating Variance\n",
    "#bias = SSE - variance\n",
    "  # O/P : 28.482148757577583\n",
    "\n",
    "#print(variance)\n",
    "#print(SSE)\n",
    "#print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from arch import arch_model\n",
    "\n",
    "# Define model\n",
    "##model = arch_model(train, mean='Zero', vol='ARCH', p=15)\n",
    "# Fit model\n",
    "##model_fit = model.fit()\n",
    "# Forecast the test set\n",
    "#yhat = model_fit.forecast(horizon=n_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pm4py\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    log = pm4py.read_xes('BPI_Challenge_2012.xes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map = pm4py.discover_heuristics_net(log)\n",
    "#pm4py.view_heuristics_net(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "#enc.fit(df[[\"concept:name\"]])\n",
    "\n",
    "#array = enc.transform(df[[\"concept:name\"]]).toarray()\n",
    "\n",
    "#df_one_hot = pd.DataFrame(array, columns = enc.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.merge(df_one_hot, left_index = True, right_index = True)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#X = df[[enc.get_feature_names_out()]]\n",
    "#y = df[\"Duration\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train linear regression model\n",
    "#model = LinearRegression()\n",
    "#model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "#y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "#mse = mean_squared_error(y_test, y_pred)\n",
    "#print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_series = df.groupby(\"position\")[[\"duration\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Define ARIMA parameters\n",
    "p = 1  # AR order\n",
    "d = 0  # Integrated order (differencing)\n",
    "q = 1  # MA order\n",
    "\n",
    "# Initialize TimeSeriesSplit cross-validator\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_arima_model(train, test, order):\n",
    "    history = list(train[\"duration\"])\n",
    "    predictions = []\n",
    "    for t in range(len(test)):\n",
    "        model = ARIMA(history, order=order)\n",
    "        model_fit = model.fit()\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(test.iloc[t][\"duration\"])\n",
    "    error = mean_absolute_error(test[\"duration\"], predictions)\n",
    "    return error\n",
    "\n",
    "# Perform k-fold cross-validation with ARIMA model using MAE\n",
    "errors = []\n",
    "for train_idx, test_idx in tscv.split(df_time_series):\n",
    "    train, test = df_time_series.iloc[train_idx], df_time_series.iloc[test_idx]\n",
    "    error = evaluate_arima_model(train, test, order=(p, d, q))\n",
    "    errors.append(error)\n",
    "\n",
    "# Calculate mean absolute error across folds\n",
    "mean_error = np.mean(errors)\n",
    "print(\"Mean Absolute Error (MAE):\", mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mae_hours = mean_error / (1000*60*60)\n",
    "mean_mae_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_series.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "# Define ARIMA parameters\n",
    "p = 1  # AR order\n",
    "d = 0  # Integrated order (differencing)\n",
    "q = 1  # MA order\n",
    "\n",
    "# Initialize TimeSeriesSplit cross-validator\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Initialize dictionary to store MAE for each variable combination\n",
    "mae_dict = {}\n",
    "\n",
    "# Define a function to evaluate ARIMA model using cross-validation with MAE\n",
    "def evaluate_arima_model(train, test, order, variables):\n",
    "    history = list(train[\"duration\"])\n",
    "    predictions = []\n",
    "    for t in range(len(test)):\n",
    "        model = ARIMA(history, order=order)\n",
    "        model_fit = model.fit()\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(test.iloc[t][\"duration\"])\n",
    "    error = mean_absolute_error(test[\"duration\"], predictions)\n",
    "    mae_dict[tuple(variables)] = mae_dict.get(tuple(variables), []) + [error]\n",
    "    return error\n",
    "\n",
    "# Perform k-fold cross-validation with ARIMA model using MAE for different feature combinations\n",
    "for train_idx, test_idx in tscv.split(df):\n",
    "    train, test = df.iloc[train_idx], df.iloc[test_idx]\n",
    "    # Generate all possible feature combinations\n",
    "    features = df.columns.drop(['date',])  # Exclude date and target value columns\n",
    "    for r in range(1, len(features) + 1):\n",
    "        for combo in combinations(features, r):\n",
    "            error = evaluate_arima_model(train, test, order=(p, d, q), variables=combo)\n",
    "\n",
    "# Calculate mean MAE for each feature combination\n",
    "mean_mae_dict = {variables: np.mean(errors) for variables, errors in mae_dict.items()}\n",
    "print(\"Mean Absolute Error (MAE) for each feature combination:\")\n",
    "for variables, mean_mae in mean_mae_dict.items():\n",
    "    print(f\"Variables: {variables}, Mean MAE: {mean_mae}\")\n",
    "    \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mae_hours = mean_error / (1000*60*60)\n",
    "mean_mae_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_2018 = pm4py.read_xes(\"BPI Challenge 2018.xes\")\n",
    "#df_2018 = pm4py.convert_to_dataframe(log_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
