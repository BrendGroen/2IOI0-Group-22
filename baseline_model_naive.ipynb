{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This files contains the implementation of the naive way of implementing the model for 3 datasets BPI_Challenge_2012/17/18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPI_Challenge_2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2012 = pd.read_csv('data/preprocessed/BPI_Challenge_2012.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2012.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning the most frequent next event in each position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finidng the most frequent next activity for each position\n",
    "position_dict = {}\n",
    "for j in range(1, max(df_2012[\"position\"]+1)):\n",
    "    dic = {}\n",
    "    for i in (df_2012[df_2012['position'] == j]).index:\n",
    "        if df_2012['next_concept:name'][i] in dic:\n",
    "            dic[df_2012['next_concept:name'][i]] += 1\n",
    "        else:\n",
    "            dic[df_2012['next_concept:name'][i]] = 1\n",
    "    \n",
    "    position_dict[j] = max(dic, key=dic.get)\n",
    "# position_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through the dataframe and map the values of position_dict to the column predicted_next_activity with lambda\n",
    "df_2012['predicted_next_concept:name'] = df_2012['position'].map(lambda x: position_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the accuracy, precision, recall and f1-score of the model separately without report\n",
    "accuracy_2012 = accuracy_score(df_2012[\"next_concept:name\"], df_2012[\"predicted_next_concept:name\"])\n",
    "precision_2012 = precision_score(df_2012[\"next_concept:name\"], df_2012[\"predicted_next_concept:name\"], average='macro', zero_division=0)\n",
    "recall_2012 = recall_score(df_2012[\"next_concept:name\"], df_2012[\"predicted_next_concept:name\"], average='macro', zero_division=0)\n",
    "f1_2012 = f1_score(df_2012[\"next_concept:name\"], df_2012[\"predicted_next_concept:name\"], average='macro', zero_division=0)\n",
    "\n",
    "print('Macro result 2012:')\n",
    "print(\"Accuracy: {:.2f}, Precision: {:.2f}, Recall: {:.2f}, F1: {:.2f}\".format(accuracy_2012, precision_2012, recall_2012, f1_2012))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the classification report\n",
    "print('Classification report:')\n",
    "print(classification_report(df_2012[\"next_concept:name\"], df_2012[\"predicted_next_concept:name\"], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPI_Challenge_2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017 = pd.read_csv('data/preprocessed/BPI_Challenge_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning the most frequent next event in each position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finidng the most frequent next activity for each position\n",
    "position_dict = {}\n",
    "for j in range(1, max(df_2017[\"position\"]+1)):\n",
    "    dic = {}\n",
    "    for i in (df_2017[df_2017['position'] == j]).index:\n",
    "        if df_2017['next_concept:name'][i] in dic:\n",
    "            dic[df_2017['next_concept:name'][i]] += 1\n",
    "        else:\n",
    "            dic[df_2017['next_concept:name'][i]] = 1\n",
    "    \n",
    "    position_dict[j] = max(dic, key=dic.get)\n",
    "# position_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through the dataframe and map the values of position_dict to the column predicted_next_activity with lambda\n",
    "df_2017['predicted_next_concept:name'] = df_2017['position'].map(lambda x: position_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the accuracy, precision, recall and f1-score of the model separately without report\n",
    "accuracy_2017 = accuracy_score(df_2017[\"next_concept:name\"], df_2017[\"predicted_next_concept:name\"])\n",
    "precision_2017 = precision_score(df_2017[\"next_concept:name\"], df_2017[\"predicted_next_concept:name\"], average='macro', zero_division=0)\n",
    "recall_2017 = recall_score(df_2017[\"next_concept:name\"], df_2017[\"predicted_next_concept:name\"], average='macro', zero_division=0)\n",
    "f1_2017 = f1_score(df_2017[\"next_concept:name\"], df_2017[\"predicted_next_concept:name\"], average='macro', zero_division=0)\n",
    "\n",
    "print('Macro result 2017:')\n",
    "print(\"Accuracy: {:.2f}, Precision: {:.2f}, Recall: {:.2f}, F1: {:.2f}\".format(accuracy_2017, precision_2017, recall_2017, f1_2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the classification report\n",
    "print('Classification report:')\n",
    "print(classification_report(df_2017[\"next_concept:name\"], df_2017[\"predicted_next_concept:name\"], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPI_Challenge_2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018 = pd.read_csv('data/preprocessed/BPI_Challenge_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning the most frequent next event in each position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finidng the most frequent next activity for each position\n",
    "position_dict = {}\n",
    "for j in range(1, max(df_2018[\"position\"]+1)):\n",
    "    dic = {}\n",
    "    for i in (df_2018[df_2018['position'] == j]).index:\n",
    "        if df_2018['next_concept:name'][i] in dic:\n",
    "            dic[df_2018['next_concept:name'][i]] += 1\n",
    "        else:\n",
    "            dic[df_2018['next_concept:name'][i]] = 1\n",
    "    \n",
    "    position_dict[j] = max(dic, key=dic.get)\n",
    "# position_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through the dataframe and map the values of position_dict to the column predicted_next_activity with lambda\n",
    "df_2018['predicted_next_concept:name'] = df_2018['position'].map(lambda x: position_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the accuracy, precision, recall and f1-score of the model separately without report\n",
    "accuracy_2018 = accuracy_score(df_2018[\"next_concept:name\"], df_2018[\"predicted_next_concept:name\"])\n",
    "precision_2018 = precision_score(df_2018[\"next_concept:name\"], df_2018[\"predicted_next_concept:name\"], average='macro', zero_division=0)\n",
    "recall_2018 = recall_score(df_2018[\"next_concept:name\"], df_2018[\"predicted_next_concept:name\"], average='macro', zero_division=0)\n",
    "f1_2018 = f1_score(df_2018[\"next_concept:name\"], df_2018[\"predicted_next_concept:name\"], average='macro', zero_division=0)\n",
    "\n",
    "print('Macro result 2018:')\n",
    "print(\"Accuracy: {:.2f}, Precision: {:.2f}, Recall: {:.2f}, F1: {:.2f}\".format(accuracy_2018, precision_2018, recall_2018, f1_2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the classification report\n",
    "print('Classification report:')\n",
    "print(classification_report(df_2018[\"next_concept:name\"], df_2018[\"predicted_next_concept:name\"], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe for the results\n",
    "df = pd.DataFrame({'2012': [accuracy_2012, precision_2012, recall_2012, f1_2012],\n",
    "                   '2017': [accuracy_2017, precision_2017, recall_2017, f1_2017],\n",
    "                   '2018': [accuracy_2018, precision_2018, recall_2018, f1_2018]},\n",
    "                  index=['Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "\n",
    "# Plotting the results\n",
    "df.plot(kind='bar', figsize=(10, 6))\n",
    "plt.ylabel('Score')\n",
    "plt.title('Comparison of the result of the naive model on the 2012, 2017, and 2018 datasets')\n",
    "plt.savefig('figs/naive_model_comparison.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
