{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter file is designed to conduct an in-depth analysis of the dataset, incorporating statistical insights and visualization. Throughout the file, we show various visual representations and numerical summaries. The final step involves saving the modified dataset in CSV format for further utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of BPI_Challenge_2012 data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "df = pd.read_csv(\"data/preprocessed/BPI_Challenge_2012.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Summary of Data (All the datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the nr rows and columns\n",
    "print(f'nr Rows: {df.shape[0]}, nr Cols: {df.shape[1]}\\n')\n",
    "\n",
    "# checking the data types\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the missing values\n",
    "df.isnull().sum().tail(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the percentage of missing values for each column\n",
    "missing = df.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "missing = missing / df.shape[0] * 100\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of unique values for each attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the unique values\n",
    "for col in df.columns:\n",
    "    print(f'{col}: {df[col].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the unique values for the 'concept:name' column\n",
    "df['concept:name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the unique values of the lifecycle:transition\n",
    "df['lifecycle:transition'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the freq of the lifecycle:transition values\n",
    "plt.figure(figsize=(10, 5))\n",
    "df['lifecycle:transition'].value_counts().plot(kind='bar')\n",
    "plt.title('Freq of lifecycle:transition')   \n",
    "plt.xlabel('lifecycle:transition')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=0)\n",
    "plt.savefig('figs/freq_lifecycle_transition.png')\n",
    "plt.show()\n",
    "\n",
    "print(df['lifecycle:transition'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the freq of the concept:name values\n",
    "plt.figure(figsize=(20, 10))\n",
    "df['concept:name'].value_counts().plot(kind='bar')\n",
    "for i, v in enumerate(df['concept:name'].value_counts()):\n",
    "    plt.text(i, v + 0.2, str(v), ha='center', va='bottom')\n",
    "plt.xlabel('concept:name', fontsize=15)\n",
    "plt.ylabel('Frequency', fontsize=15)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Freq of concept:name', fontsize=15)  \n",
    "# increase the font size\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/freq_concept_name.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the unique values for the case:concept:name column\n",
    "df['case:concept:name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_dict = {}\n",
    "for j in range(1, max(df[\"position\"]+1)):\n",
    "    dic = {}\n",
    "    for i in (df[df['position'] == j]).index:\n",
    "        if df['concept:name'][i] in dic:\n",
    "            dic[df['concept:name'][i]] += 1\n",
    "        else:\n",
    "            dic[df['concept:name'][i]] = 1\n",
    "    \n",
    "    position_dict[j] = max(dic, key=dic.get)\n",
    "\n",
    "nr_positions_dominant_in = {}\n",
    "\n",
    "for i in position_dict.items():\n",
    "    if i[1] in nr_positions_dominant_in:\n",
    "        nr_positions_dominant_in[i[1]] += 1\n",
    "    else:\n",
    "        nr_positions_dominant_in[i[1]] = 1\n",
    "\n",
    "# printing the dominant activity in each position in percentage\n",
    "for i in nr_positions_dominant_in.items():\n",
    "    print(f'{i[0]}: {i[1]/max(df[\"position\"])*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis specific to the BPI_Challenge_2018 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/preprocessed/BPI_Challenge_2018.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the percentage of missing values for each column\n",
    "missing = df.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "missing = missing / df.shape[0] * 100\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding all the columns that have values either True or False\n",
    "bool_cols = [col for col in df.columns if np.isin(df[col].dropna().unique(), [True, False]).all()]\n",
    "bool_cols, len(bool_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the boolean_columns from the dataframe\n",
    "df_no_bool = df.drop(columns=bool_cols)\n",
    "df_no_bool.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_bool.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the rows that have the value '0;n/a' in the 'org:resource' column since isna() does not work for this value\n",
    "df_no_bool[df_no_bool['org:resource'] == '0;n/a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the isna() method does not capture 0;n/a as missing values, we need to replace it with np.nan\n",
    "df_no_bool['org:resource'] = df_no_bool['org:resource'].replace('0;n/a', np.nan)\n",
    "df_no_bool['org:resource'].isna().sum()/len(df_no_bool) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the value counts for the subprocess column\n",
    "df_no_bool['subprocess'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the freq of the subprocess values\n",
    "plt.figure(figsize=(20, 10))\n",
    "df_no_bool['subprocess'].value_counts().plot(kind='bar')\n",
    "for i, v in enumerate(df_no_bool['subprocess'].value_counts()):\n",
    "    plt.text(i, v + 0.2, str(v), ha='center', va='bottom')\n",
    "plt.title('Freq of subprocess')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the boolean columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dataframe with only the boolean columns\n",
    "df_bool = df[bool_cols]\n",
    "\n",
    "# checking for the number of unique values for each column\n",
    "unique = df_bool.nunique()\n",
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drpoping the case:greening and case:basic payment columns since they have only one unique value\n",
    "df_bool = df_bool.drop(columns=['case:greening', 'case:basic payment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the similarity between the boolean columns\n",
    "similarity = df_bool.corr()\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the columns that have a correlation above 0.7 and they similarity\n",
    "corr_cols = []\n",
    "for i in range(similarity.shape[0]):\n",
    "    for j in range(i+1, similarity.shape[0]):\n",
    "        if abs(similarity.iloc[i, j]) > 0.5:\n",
    "            corr_cols.append((similarity.columns[i], similarity.columns[j], similarity.iloc[i, j] ))\n",
    "corr_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the similarity matrix\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.matshow(similarity, cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title('Similarity Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_bool['case:amount_applied3'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_no_bool['case:risk_factor'].value_counts() / len(df_no_bool) * 100)\n",
    "\n",
    "# Dropping the risk factor column since it does not contain much information\n",
    "df_no_bool = df_no_bool.drop(columns=['case:risk_factor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_no_bool['case:cross_compliance'].value_counts()/len(df_no_bool) * 100)\n",
    "\n",
    "# Dropping the cross_compliance column since it does not contain much information\n",
    "df_no_bool = df_no_bool.drop(columns=['case:cross_compliance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['case:program-id'].value_counts() / len(df) * 100)\n",
    "\n",
    "# Dropping the program-id column since it does not contain much information\n",
    "df_no_bool = df_no_bool.drop(columns=['case:program-id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropping_columns = ['case:penalty_amount1', 'case:penalty_amount2', 'case:penalty_amount3', 'case:payment_actual1',\n",
    "                    'case:payment_actual2', 'case:payment_actual3', 'case:amount_applied1', 'case:amount_applied2', 'case:amount_applied3']\n",
    "\n",
    "# Dropping the above columns since they do not contain much information due to the high percentage of missing values\n",
    "df_no_bool = df_no_bool.drop(columns=dropping_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking what columns are constant across the same case:concept:name\n",
    "df_no_bool.groupby('case:concept:name').nunique().nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between the columns case:amount_applied0 and case:amount_actual0\n",
    "print(df_no_bool[['case:amount_applied0', 'case:payment_actual0']].corr())\n",
    "\n",
    "# dropping one of the columns since they are highly correlated\n",
    "df_no_bool = df_no_bool.drop(columns=['case:payment_actual0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the boolean columns that are highly correlated with other columns\n",
    "df_bool = df_bool.drop(columns=['case:penalty_AVBP', 'case:penalty_AGP', 'case:selected_random'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final columns to be used for the further steps\n",
    "df_no_bool.columns, df_bool.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
