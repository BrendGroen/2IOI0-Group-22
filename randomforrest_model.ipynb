{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter notebook we tried to train a simple Radom Forrest model on the 3 datasets BPI Challenge 2012, 2017, and 2018. We mostly value the result of how well Random Forrest performed on the 3 models, but we also want to observe how inclusion of more features and feeding more data to the model could impact the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from aux_functions import split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = 0.8\n",
    "rng = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPI Challenge 2012 Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2012 = pd.read_csv('data/preprocessed/BPI_Challenge_2012.csv')\n",
    "df_2012.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into train and test\n",
    "df_train_2012, df_test_2012 = split_data(df_2012, ratio=0.8, report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2012.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest Classifier. By convention, clf means 'Classifier'\n",
    "clf_2012 = RandomForestClassifier(n_jobs=2, random_state=0)\n",
    "\n",
    "# specifying the predictor columns and the target column\n",
    "predictor_columns_2012 = ['position', 'concept:name', 'lifecycle:transition', 'case:AMOUNT_REQ']\n",
    "target_column_2012 = 'next_concept:name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing one-hot encoding on columns\n",
    "# X_train_2012 = pd.get_dummies(df_train_2012[predictor_columns_2012])\n",
    "# y_train_2012 = df_train_2012[target_column_2012]\n",
    "# X_train_2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of using the one-hot encoding, we can use the label encoding\n",
    "le = LabelEncoder()\n",
    "X_train_2012 = df_train_2012[predictor_columns_2012].apply(le.fit_transform)\n",
    "y_train_2012 = df_train_2012[target_column_2012]\n",
    "X_train_2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Classifier to take the training features and learn how they relate\n",
    "# to the training y\n",
    "clf_2012.fit(X_train_2012, y_train_2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the model\n",
    "# X_test_2012 = pd.get_dummies(df_test_2012[predictor_columns_2012])\n",
    "# y_test_2012 = df_test_2012[target_column_2012]\n",
    "# y_pred_2012 = clf_2012.predict(X_test_2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of using the one-hot encoding, we can use the label encoding\n",
    "X_test_2012 = df_test_2012[predictor_columns_2012].apply(le.fit_transform)\n",
    "y_test_2012 = df_test_2012[target_column_2012]\n",
    "y_pred_2012 = clf_2012.predict(X_test_2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n",
    "accuracy = accuracy_score(y_test_2012, y_pred_2012)\n",
    "# printing f1 score\n",
    "f1 = f1_score(y_test_2012, y_pred_2012, average='weighted')\n",
    "# printing precision score\n",
    "precision = precision_score(y_test_2012, y_pred_2012, average='weighted')\n",
    "# printing recall score\n",
    "recall = recall_score(y_test_2012, y_pred_2012, average='weighted')\n",
    "\n",
    "f1, precision, recall, accuracy\n",
    "f'f1: {f1}, precision: {precision}, recall: {recall}, accuracy: {accuracy}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting the confusion matrix with seaborn\n",
    "\n",
    "\n",
    "conf_mat = confusion_matrix(y_test_2012, y_pred_2012)\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=clf_2012.classes_, yticklabels=clf_2012.classes_)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_2012, df_train_2012, df_test_2012, X_train_2012, y_train_2012, X_test_2012, y_test_2012, y_pred_2012, clf_2012, predictor_columns_2012, target_column_2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO NOT RUN THE CODE BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPI Challenge 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018 = pd.read_csv('data/preprocessed/BPI_Challenge_2018.csv')\n",
    "df_2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "# encode the concept:name as well\n",
    "df_2018['concept:name_encoded'] = le.fit_transform(df_2018['concept:name'])\n",
    "df_2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining penalty columns\n",
    "bolean_columns = ['success',\n",
    "  'case:young farmer',\n",
    "  'case:selected_random',\n",
    "  'case:penalty_AJLP',\n",
    "  'case:penalty_BGKV',\n",
    "  'case:penalty_AUVP',\n",
    "  'case:small farmer',\n",
    "  'case:penalty_BGP',\n",
    "  'case:penalty_C16',\n",
    "  'case:penalty_BGK',\n",
    "  'case:penalty_AVUVP',\n",
    "  'case:penalty_CC',\n",
    "  'case:penalty_AVJLP',\n",
    "  'case:penalty_C9',\n",
    "  'case:rejected',\n",
    "  'case:greening',\n",
    "  'case:penalty_C4',\n",
    "  'case:penalty_AVGP',\n",
    "  'case:penalty_ABP',\n",
    "  'case:penalty_B6',\n",
    "  'case:penalty_B4',\n",
    "  'case:penalty_B5',\n",
    "  'case:penalty_AVBP',\n",
    "  'case:penalty_B2',\n",
    "  'case:selected_risk',\n",
    "  'case:penalty_B3',\n",
    "  'case:selected_manually',\n",
    "  'case:penalty_AGP',\n",
    "  'case:penalty_B16',\n",
    "  'case:penalty_GP1',\n",
    "  'case:basic payment',\n",
    "  'case:penalty_B5F',\n",
    "  'case:penalty_V5',\n",
    "  'case:redistribution',\n",
    "  'case:penalty_JLP6',\n",
    "  'case:penalty_JLP7',\n",
    "  'case:penalty_JLP5',\n",
    "  'case:penalty_JLP2',\n",
    "  'case:penalty_JLP3',\n",
    "  'case:penalty_JLP1']\n",
    "\n",
    "int_columns = ['case:cross_compliance', 'case:area', 'case:payment_actual0', 'case:amount_applied0', 'case:year', 'case:number_parcels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying the predictor columns and the target column\n",
    "predictor_columns_2018 = ['position', 'concept:name', 'success'] + int_columns\n",
    "target_column_2018 = 'next_concept:name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the column names for one-hot encoding\n",
    "predictor_plus_onehot_columns_2018 = pd.get_dummies(df_2018[predictor_columns_2018]).columns.to_list() + int_columns\n",
    "print(predictor_plus_onehot_columns_2018)\n",
    "\n",
    "# add the one-hot encoding for the predictor and add them to the dataframe and remove the original columns\n",
    "df_2018 = pd.concat([df_2018, pd.get_dummies(df_2018[predictor_columns_2018])], axis=1)\n",
    "df_2018.drop(columns=['concept:name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into train and test\n",
    "df_train_2018, df_test_2018 = split_data(df_2018, ratio=0.65, report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest Classifier. By convention, clf means 'Classifier'\n",
    "clf_2018 = RandomForestClassifier(n_jobs=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying the target column\n",
    "target_column_2018 = 'next_concept:name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing one-hot encoding on columns\n",
    "X_train_2018 = df_train_2018[predictor_plus_onehot_columns_2018]\n",
    "y_train_2018 = df_train_2018[target_column_2018]\n",
    "X_train_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of one-hot icodeing lets implement le encoding\n",
    "# X_train_2018 = df_train_2018[predictor_columns_2018].apply(le.fit_transform)\n",
    "# y_train_2018 = df_train_2018[target_column_2018]\n",
    "# X_train_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Classifier to take the training features and learn how they relate\n",
    "# to the training y\n",
    "\n",
    "clf_2018.fit(X_train_2018, y_train_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_2018.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_2018 = pd.get_dummies(df_train_2018[predictor_columns_2018], dtype='uint8', sparse=True)\n",
    "# y_train_2018 = df_train_2018[target_column_2018]\n",
    "# X_train_2018\n",
    "\n",
    "# evaluating the model\n",
    "X_test_2018 = pd.get_dummies(df_test_2018[predictor_columns_2018], dtype='uint8', sparse=True)\n",
    "y_test_2018 = df_test_2018[target_column_2018]\n",
    "y_pred_2018 = clf_2018.predict(X_test_2018)\n",
    "y_train_2018\n",
    "\n",
    "X_train_2018 = df_train_2018[predictor_plus_onehot_columns_2018]\n",
    "y_train_2018 = df_train_2018[target_column_2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of one-hot icodeing lets implement le encoding\n",
    "# X_test_2018 = df_test_2018[predictor_columns_2018].apply(le.fit_transform)\n",
    "# y_test_2018 = df_test_2018[target_column_2018]\n",
    "# y_pred_2018 = clf_2018.predict(X_test_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n",
    "accuracy = accuracy_score(y_test_2018, y_pred_2018)\n",
    "# printing f1 score\n",
    "f1 = f1_score(y_test_2018, y_pred_2018, average='weighted')\n",
    "# printing precision score\n",
    "precision = precision_score(y_test_2018, y_pred_2018, average='weighted')\n",
    "# printing recall score\n",
    "recall = recall_score(y_test_2018, y_pred_2018, average='weighted')\n",
    "\n",
    "f1, precision, recall, accuracy\n",
    "f'f1: {f1}, precision: {precision}, recall: {recall}, accuracy: {accuracy}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the coefs and what columns are the most important\n",
    "importances = clf_2018.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf_2018.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking with the feature name\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X_train_2018.shape[1]):\n",
    "    print(f\"{f+1}. feature {X_train_2018.columns[indices[f]]} ({importances[indices[f]]})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
